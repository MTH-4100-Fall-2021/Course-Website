---
title: "Poisson model (wrap-up); Monte Carlo approximation and sampling"
author: "Dr. Olanrewaju Michael Akande"
date: "Jan 24, 2020"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
```


## Announcements

- HW1 due midnight

- HW2 now online, due next Thursday.
  + There should be six questions.
  + If you only see five questions, refresh your browser.
  

---
## Outline

- Poisson model (wrap-up)
  + Example
  + Posterior prediction
  + Other parameterizations
  
- Finding conjugate distributions

- Monte Carlo approximation

- Sampling methods
  + Simple accept/reject
  + Importance sampling
  


---
class: center, middle

# Poisson model (wrap-up)


---
## Poisson-Gamma recap

Poisson data:
.block[
.small[
$$f(y_i; \theta): y_1,\ldots,y_n \overset{iid}{\sim} \textrm{Po}(\theta)$$
]
]


--

$+$ Gamma Prior:
.block[
.small[
$$\pi(\theta) = \frac{b^a}{\Gamma(a)} \theta^{a-1}e^{-b\theta} = \textrm{Ga}(a,b)$$
]
]

--

$\Rightarrow$ Gamma posterior:
.block[
.small[
$$\pi(\theta | \{y_i\}): \theta | \{y_i\} \sim \textrm{Ga}(a+\sum y_i,b+n).$$
]
]

--

- Recall: for $\textrm{Gamma}(a,b)$,
  + $\mathbb{E}[\theta] = \dfrac{a}{b}$
  + $\mathbb{V}[\theta] = \dfrac{a}{b^2}$
  + $\textrm{Mode}[\theta] = \dfrac{a-1}{b} \ \ \textrm{for} \ \ a \geq 1$


---
## Hoff example: birth rates

- Survey data on educational attainment and number of children of 155 forty-year-old women during the 1990's.

--

- These women were in their 20s during the 1970s, a period of historically low fertility rates in the US.

--

- **Goal**: compare birth rate $\theta_1$ for women with bachelor's degrees to the rate $\theta_2$ for women without.

--

- **Data**:
  + 111 women without a bachelor's degree had 217 children: $(\bar{y}_1 = 1.95)$
  + 44 women with bachelor's degrees had 66 children: $(\bar{y}_2 = 1.50)$
  
--

- Based on the data alone, looks like $\theta_1$ should be greater than $\theta_2$. But...how sure are we?
  
--

- **Priors**: $\theta_1, \theta_2 \sim \textrm{Ga}(2,1)$ (not much prior information; equivalent to 1 prior woman with 2 children). Posterior means will be close to the MLEs.



---
## Hoff example: birth rates

- Then,
  + $\theta_1 | \{n_1=111, \sum y_{i,1}=217\} \sim \textrm{Ga}(2+217,1+111) = \textrm{Ga}(219,112).$
  
  + $\theta_2 | \{n_2=44, \sum y_{i,2}=66\} \sim \textrm{Ga}(2+66,1+44) = \textrm{Ga}(68,45).$
  
--

- Use R to calculate posterior means and 95% CIs for $\theta_1$ and $\theta_2$.
    ```{r eval=F}
a=2; b=1; #prior
n1=111; sumy1=217; n2=44; sumy2=66 #data
(a+sumy1)/(b+n1); (a+sumy2)/(b+n2); #post means
qgamma(c(0.025, 0.975),a+sumy1,b+n1) #95\% ci 1
qgamma(c(0.025, 0.975),a+sumy2,b+n2) #95\% ci 2
```

--

- Posterior means: $\mathbb{E}[\theta_1 | \{y_{i,1}\}] = 1.955$ and $\mathbb{E}[\theta_2 | \{y_{i,2}\}] = 1.511$.
  
--

- 95% credible intervals
  + $\theta_1$: [1.71, 2.22].
  + $\theta_2$: [1.17, 1.89].
  


---
## Hoff example: birth rates

Prior and posteriors:

```{r fig.height=5, echo=F}
x <- seq(0,6,by=0.00001)
plot(x,dgamma(x,shape=2,rate=1),xlim=c(0,6),ylim=c(0,3),lwd=1.5,type="l",
      col="green3",xlab=expression(theta),ylab="density")
lines(x,dgamma(x,shape=219,rate=112),col="red2",lwd=1.5,type="l")
lines(x,dgamma(x,shape=68,rate=45),col="blue2",lwd=1.5,type="l")
legend("topright", legend=c("Prior","Post. (No Bach)","Post. (Bach)"),
       col=c("green3","red2","blue2"), lwd=2, cex=1)
```




---
## Hoff example: birth rates

- Posteriors indicate considerable evidence birth rates are higher among women without bachelor's degrees.

--

- Confirms what we observed.

--

- Using sampling we can quickly calculate $\Pr(\theta_1 > \theta_2 | \textrm{data})$.
    ```{r eval=F}
mean(rgamma(10000,219,112)>rgamma(10000,68,45))
```

  We have $\Pr(\theta_1 > \theta_2 | \textrm{data}) = 0.97$.
  
--

- Why/how does it work?

--

- .hlight[Monte Carlo approximation] coming soon!

--

- Clearly, that probability will change with different priors.



---
## Posterior predictive distribution

- <div class="question">
What is the posterior predictive distribution for the Poisson-gamma model?
</div>

--

- Let $a_n = a+\sum y_i$ and $b_n = b+n$.

--

- We have
.block[
.small[
$$
\begin{aligned}
f(y_{n+1}|y_{1:n}) &= \int f(y_{n+1}|\theta) \pi(\theta|y_{1:n})\,d\theta \\
&= \int \textrm{Po}(y_{n+1}; \theta)  \textrm{Ga}(\theta; a_n,b_n)\,d\theta \\
&= ... \\
&= ... \\
&= \dfrac{\Gamma(a_n + y_{n+1})}{\Gamma(a_n)\Gamma(y_{n+1}+1)} \ \left(\dfrac{b_n}{b_n + 1} \right)^{a_n} \ \left(\dfrac{1}{b_n + 1} \right)^{y_{n+1}}
\end{aligned}
$$
]
]

  which is the .hlight[negative binomial distribution], $\textrm{Neg-binomial}\left(a_n,\dfrac{1}{b_n + 1}\right)$.
  
--

- The .hlight[prior predictive distribution] $f(y_{n+1}; a,b)$  takes a similar form.



---
## Negative binomial distribution

- Originally derived as the number of successes in a sequence of independent $\textrm{Bernoulli}(p)$ trials before $r$ failures occur.

--

- The negative binomial distribution $\textrm{Neg-binomial}\left(r,p\right)$ is parameterized by $r$ and $p$ and the pmf is given by
.block[
.small[
$$\Pr[Y = y | r, p] = {y + r - 1 \choose y} (1-p)^rp^y; \ \ \ \ y=0,1,2,\ldots; \ \ \ p \in [0,1].$$
]
]

- Starting with this, the distribution can be extended to allow $r \in (0,\infty)$ as
.block[
.small[
$$\Pr[Y = y | r, p] = \dfrac{\Gamma(y+r)}{\Gamma(y+1)\Gamma(r)} (1-p)^rp^y; \ \ \ \ y=0,1,2,\ldots; \ \ \ p \in [0,1].$$
]
]

--

- Some properties:

  + $\mathbb{E}[\theta] = \dfrac{pr}{1-p}$
  
  + $\mathbb{V}[\theta] = \dfrac{pr}{(1-p)^2}$




---
## Posterior predictive distribution

- The negative binomial distribution is an over-dispersed generalization of the Poisson.

--

- <div class="question">
What does over-dispersion mean?
</div>

--

- In marginalizing $\theta$ out of the Poisson likelihood, over a gamma distribution, we obtain a negative-binomial.

--

- For $(y_{n+1}|y_{1:n}) \sim \textrm{Neg-binomial}\left(a_n,\dfrac{1}{b_n + 1}\right)$, we have

  + $\mathbb{E}[y_{n+1}|y_{1:n}] = \dfrac{a_n}{b_n} = \mathbb{E}[\theta|y_{1:n}] =$ posterior mean, and
  
  + $\mathbb{V}[y_{n+1}|y_{1:n}] = \dfrac{a_n(b_n+1)}{b_n^2} = \mathbb{E}[\theta|y_{1:n}] \left(\dfrac{b_n+1}{b_n}\right)$,
  
    so that variance is larger than the mean by an amount determined by $b_n$, which takes the over-dispersion into account.



---
## Predictive uncertainty

- Note that as the sample size $n$ increases, the posterior density for $\theta$ becomes more and more concentrated.
.block[
.small[
$$\mathbb{V}[\theta | y_{1:n}] = \dfrac{a_n}{b_n^2} = \dfrac{a + \sum_i y_i}{(b + n)^2} \approx \dfrac{\bar{y}}{n} \rightarrow 0.$$
]
]

--

- Also, recall that $\mathbb{V}[y_{n+1}|y_{1:n}] = \mathbb{E}[\theta] \left(\dfrac{b_n+1}{b_n}\right)$.

--

- As we have less uncertainty about $\theta$, the inflation factor
.block[
.small[
$$\dfrac{b_n+1}{b_n} = \dfrac{b + n+1}{b + n} \rightarrow 1$$
]
]

  and the predictive density $f(y_{n+1}|y_{1:n}) \rightarrow \textrm{Po}(\bar{y})$.
  
--

- Of course, in smaller samples, it is important to inflate our predictive intervals to account for uncertainty in $\theta$.



---
## Back to birth rates

- Let's compare the posterior predictive distributions for the two groups of women.
```{r fig.height=4.9, echo=F}
a=2; b=1; #prior
n1=111; sumy1=217; n2=44; sumy2=66 #data
y=0:10
ltbach=dnbinom(y,size=(a+sumy1),mu=(a+sumy1)/(b+n1))
bach=dnbinom(y,size=(a+sumy2),mu=(a+sumy2)/(b+n2))
barplot(rbind(ltbach,bach),xlab="Children", 
        names.arg=c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
        col=c("orange3","green3"),beside=TRUE, main="Posterior predictive distributions")
legend("topright",c("No Bachelors","Bachelors"),bty="n", fill=c("orange3","green3"))
```



---
## Poisson model in terms of rate

- In many applications, it is often convenient to parameterize the Poisson model a bit differently. One option takes the form
.block[
.small[
$$y_i \sim \textrm{Po}(x_i\theta); \ \ \ i=1,\ldots, n.$$
]
]

  where $x_i$ represents an explanatory variable and $\theta$ is once again the population parameter of interest. The model is not exchangeable in the $y_i$'s but is exchangeable in the pairs $(x,y)_i$.
  
--

- In epidemiology, $\theta$ is often called the population "rate" and $x_i$ is called the "exposure" of unit $i$.

--

- When dealing with mortality rates in different counties for example, $x_i$ can be the population $n_i$ in county $i$, with $\theta =$ the overall mortality rate.

--

- The gamma distribution is still conjugate for $\theta$, with the resulting posterior taking the form
.block[
.small[
$$\pi(\theta | \{x_i, y_i\}): \theta | \{x_i, y_i\} \sim \textrm{Ga}(a+\sum_i y_i,b+\sum_i x_i).$$
]
]



---
## BDA example: asthma mortality rate

- Consider an example on estimating asthma mortality rates for cities in the US.

--

- Since actual mortality rates can be small on the raw scale, they are often commonly estimated per 100,000 or even per one million. 

--

- To keep it simple, let's use "per 100,000" for this example.

--

- For inference, ideally, we collect data which should basically count the number of asthma-related deaths per county.

--

- Note that inference is by county here, so county is indexes observations in the sample.

--

- Since we basically have count data, a Poisson model would be reasonable here.



---
## Asthma mortality rate

- Since each city would be expected to have different populations, we might consider the sampling model:
.block[
.small[
$$y_i \sim \textrm{Po}(x_i\theta); \ \ \ i=1,\ldots, n.$$
]
]

  where 
    + $x_i$ is the "exposure" for county $i$, that is, population of county $i$ is $x_i \times 100,000$; and
    + $\theta$ is the unknown "true" city mortality rate per 100,000.

- Suppose 
  + we pick a city in the US with population of 200,000;

--
  + we find that 3 people died of asthma, i.e., roughly 1.5 cases per 100,000.

--

- Thus, we have one single observation with $x_i = 2$ and $y_i = 3$ for this city.



---
## Asthma mortality rate

- Next, we need to specify a prior. What is a sensible prior here?

--

- Perhaps we should look at mortality rates around the world or in similar countries.

--

- Suppose reviews of asthma mortality rates around the world suggest rates above 1.5 per 100,000 are very rare in Western countries, with typical rates around 0.6 per 100,000.

--

- Let's try a gamma distribution with $\mathbb{E}[\theta] = 0.6$ and $\Pr[\theta \geq 1.5]$ very low!

--

- A few options here, but let's go with $\textrm{Ga}(3,5)$, which has $\mathbb{E}[\theta] = 0.6$ and $\Pr[\theta \geq 1.5] \approx 0.02$.

--

- Using trial-and error, explore more options in R!



---
## Asthma mortality rate

- Therefore, our posterior takes the form
.block[
.small[
$$\pi(\theta | \{x_i, y_i\}): \theta | \{x_i, y_i\} \sim \textrm{Ga}(a+\sum_i y_i,b+\sum_i x_i)$$
]
]

  which is actually
.block[
.small[
$$\pi(\theta | x,y) = \textrm{Ga}(a+y,b+x) = \textrm{Ga}(3+3,5+2) = \textrm{Ga}(6,7).$$
]
]

--

- $\mathbb{E}[\theta | x,y] = 6/7=0.86$ so that we expect less than 1 (0.86 to be exact) asthma-related deaths per 100,000 people in this city.
  
--

- In fact, the posterior probability that the long term death rate from asthma in this city is more than 1 per 100,000, $\Pr[\theta > 1| x,y]$, is 0.3.

--

- Also, $\Pr[\theta \geq 2 | x,y] = 0.99$, so that there is very little chance that we see more than 2 asthma-related deaths per 100,000 people in this city.

--

- Use `pgamma` in R to compute the probabilities.



---
## Prior vs posterior

```{r fig.height=4.2, echo=F}
x <- seq(0,3,by=0.00001)
plot(x,dgamma(x,shape=3,rate=5),xlim=c(0,3),ylim=c(0,2),lwd=1.5,type="l",
      col="orange3",xlab=expression(theta),ylab="density")
lines(x,dgamma(x,shape=6,rate=7),col="red3",lwd=1.5,type="l")
legend("topright", legend=c("Prior","Posterior"),
       col=c("orange3","red3"), lwd=2, cex=1)
```

Posterior is to the right of the prior since the data suggests higher mortality rates are more likely than the prior suggests. However, we only have one data point!



---
class: center, middle

# Finding conjugate distributions



---
## Finding conjugate distributions

- In the conjugate examples we have looked at so far, how did we know the prior distributions we chose would result in conjugacy?

--

- <div class="question">
Can we figure out the family of distributions that would be conjugate for arbitrary densities?
</div>

--

- Let's explore this using the .hlight[exponential distribution]. The exponential distribution is often used to model "waiting times" or other random variables (with support $(0,\infty)$) often measured on a time scale.

--

- If $y \sim \textrm{Exp}(\theta)$, we have the pdf
.block[
.small[
$$f(y) = \theta e^{-y\theta}; \ \ \ y > 0.$$
]
]

  where $\theta$ is the .hlight[rate parameter], and $\mathbb{E}[y] = 1/\theta$.
  
--

- Recall, if $Y \sim \textrm{Ga}(1,\theta)$, then $Y \sim \textrm{Exp}(\theta)$. What is $\mathbb{V}[y]$ then?

--

- Let's figure out what the conjugate prior for this density would look like (to be done in class).



---
class: center, middle

# Monte Carlo approximation


---
## Monte Carlo approximation

- Monte Carlo integration is very key for Bayesian computation and using simulations in general.

--

- While we will focus on using Monte Carlo integration for Bayesian inference, the development is general and applies to any pdf/pmf $p(\theta)$.

--

- For our purposes, we will want to evaluate expectations of the form
.block[
.small[
$$H = \int h(\theta) p(\theta) d\theta,$$
]
]

  for many different functions $h(.)$ (usually scalar for us).
  
--

- Procedure:
  1. Generate a random sample $\theta_1, \ldots, \theta_m \overset{ind}{\sim} p(\theta)$.
  
--
  2. Estimate $H$ using
.block[
.small[
$$\bar{h} = \dfrac{1}{m} \sum_{i=1}^m h(\theta_i).$$
]
]
  

---
## Monte Carlo approximation

- We have $\mathbb{E}[h(\theta_i)] = H$.

--

- Assuming $\mathbb{E}[h^2(\theta_i)] < \infty$, so that the variance of each $h(\theta_i)$ is finite, we have

  1. .hlight[LLN]: $\bar{h} \overset{a.s.}{\rightarrow} H$.
  
--
  2. .hlight[CLT]: $\bar{h} - H$ is is asymptotically normal, with asymptotic variance
.block[
.small[
$$\dfrac{1}{m} \int (h(\theta)-H)^2 p(\theta) d\theta,$$
]
]

      which can be approximated by
.block[
.small[
$$v_m = \dfrac{1}{m^2} \sum_{i=1}^m (h(\theta_i)-\bar{h})^2.$$
]
]

--

- $\sqrt{v_m}$ is often called the .hlight[Monte Carlo standard error].


---
## Monte Carlo approximation

- That is, generally, taking large Monte Carlo sample sizes $m$ (in the thousands or tens of thousands) can yield very precise, and cheaply computed, numerical approximations to mathematically difficult integrals.

--

- .hlight[What this means for us]: we can approximate just about any aspect of the posterior distribution with a large enough Monte Carlo sample.

--

- For samples $\theta_1, \ldots, \theta_m$ drawn iid from $p(\theta|y)$, as $m \rightarrow \infty$, we have

--
  + $\bar{\theta} = \dfrac{1}{m} \sum\limits_{i=1}^m \theta_i \rightarrow \mathbb{E}[\theta | y]$
  
--
  + $\hat{\sigma}_{\theta} = \dfrac{1}{m-1} \sum\limits_{i=1}^m (\theta_i - \bar{\theta})^2 \rightarrow \mathbb{V}[\theta | y]$

--
  + $\dfrac{1}{m} \sum\limits_{i=1}^m \mathbb{1}[\theta_i \leq c] = \dfrac{\# \theta_i \leq c}{m} \rightarrow \Pr[\theta \leq c| y]$
  
--
  + $[\dfrac{\alpha}{2}\textrm{th} \ \textrm{percentile of } (\theta_1, \ldots, \theta_m), (1-\dfrac{\alpha}{2})\textrm{th} \ \textrm{percentile of } (\theta_1, \ldots, \theta_m)]$ $\rightarrow$ $100 \times (1-\alpha)%$ quantile-based credible interval.



---
## Back to birth rates

- Suppose we randomly sample two women, one with degree and one without. To what extent do we expect the one without the degree to have more kids than the other, e.g. $\tilde{y}_1 > \tilde{y}_2$?

--

- Using R, 
    ```{r fig.height=4}
    set.seed(01222020)
a=2; b=1; #prior
n1=111; sumy1=217; n2=44; sumy2=66 #data
mean(rnbinom(100000,size=(a+sumy1),mu=(a+sumy1)/(b+n1)) >
rnbinom(10000,size=(a+sumy2),mu=(a+sumy2)/(b+n2)))
mean(rnbinom(100000,size=(a+sumy1),mu=(a+sumy1)/(b+n1))==
rnbinom(10000,size=(a+sumy2),mu=(a+sumy2)/(b+n2)))
```

- That is, $\Pr(\tilde{y}_1 > \tilde{y}_2) \approx 0.48$ and $\Pr(\tilde{y}_1 > \tilde{y}_2) \approx 0.22$.

--

- Strong evidence of difference between two populations does not really imply the difference in predictions is large.




---
## Monte Carlo approximation

- This general idea of using samples to "approximate" averages (expectations) is also useful when trying to approximate posterior predictive distributions.

--

- Quite often, we are able to sample from $f(y_i; \theta)$ and $\pi(\theta | \{y_i\})$ but not from $f(y_{n+1}|y_{1:n})$ directly.

--

- We can do so indirectly using the following Monte Carlo procedure:
.block[
.small[
$$
\begin{split}
\textrm{sample } \theta^{(1)} & \sim \pi(\theta | \{y_i\}), \ \textrm{ then sample } y_{n+1}^{(1)} \sim f(y_{n+1}; \theta^{(1)})\\
\textrm{sample } \theta^{(2)} & \sim \pi(\theta | \{y_i\}), \ \textrm{ then sample } y_{n+1}^{(2)} \sim f(y_{n+1}; \theta^{(2)})\\
& \ \ \vdots \hspace{133pt} \vdots \\
\textrm{sample } \theta^{(m)} & \sim \pi(\theta | \{y_i\}), \ \textrm{ then sample } y_{n+1}^{(m)} \sim f(y_{n+1}; \theta^{(m)}).\\
\end{split}
$$
]
]

--

- The sequence $\{(\theta, y_{n+1})^{(1)}, \ldots, (\theta, y_{n+1})^{(m)}\}$ constitutes $m$ independent samples from the joint posterior of $(\theta, Y_{n+1})$.

--

- In fact, $\{ y_{n+1}^{(1)}, \ldots, y_{n+1}^{(m)}\}$ are independent draws from the posterior predictive distribution we care about.



---
class: center, middle

# Sampling methods



---
## Rejection sampling (simple accept/reject)

- Setup:
  + $p(\theta)$ is some density we are interested in sampling from;
  
  + $p(\theta)$ is tough to sample from but we are able to evaluate $p(\theta)$ as a function at any point; and
  
  + $g(\theta)$ is some .hlight[proposal distribution] or .hlight[importance sampling distribution] that is easier to sample from.

--

- Two key requirements:
  + $g(\theta)$ is easy to sample from, and
  
  + $g(\theta)$ is easy to evaluate at any point as for $p(\theta)$.
  
--

- Usually, the context is one in which $g(\theta)$ has been derived as an analytic approximation to $p(\theta)$; and the closer the approximation, the more accurate the resulting Monte Carlo analysis will be.



---
## Rejection sampling (simple accept/reject)

- Procedure:

  1. Define $w(\theta) = p(\theta)/g(\theta)$.
  
--
  2. Assume that $w(\theta) = p(\theta)/g(\theta) < M$ for some constant M. If $g(\theta)$ represents a good approximation to $p(\theta)$, then M should not be too far from 1.

--
  3. Generate a _candidate_ value $\theta \sim g(\theta)$ and **accept** with probability $w(\theta)/M$: if accepted, $\theta$ is a draw from $p(\theta)$; otherwise **reject** and try again.
  
--
      Equivalently, generate $u \sim U(0,1)$ independently of $\theta$. Then **accept** $\theta$ as a draw from $p(\theta)$ if, and only if, $u < w(\theta)/M$.

--

- For those interested, the proof that all accepted $\theta$ values are indeed from $p(\theta)$ is on the last slide.

--

- **Drawback**: we need $M$ for this to work. However, in the case of truncated densities, we actually have $M$.



---
## Rejection sampling for truncated densities

- The inverse CDF method works well for truncated densities but what happens when we can not write down the truncated CDF?

--

- Suppose we want to sample from $f_{[a,b]}(\theta)$, that is, a known pdf $f(\theta)$ truncated to $[a,b]$.

  + Recall that $f_{[a,b]}(\theta) \propto f(\theta)\mathbb{1}[\theta \in [a,b]]$. Using the notation for rejection sampling, $p(\theta) = f_{[a,b]}(\theta)$ and $g(\theta) = f(\theta)$.
  
--
  + Set $1/M = \int^b_a f(\theta^\star)\textrm{d}\theta^\star$, so that $M$ is a constant. Then, $w(\theta) = p(\theta)/g(\theta) = M \mathbb{1}[\theta \in [a,b]] \leq M$ as required.

--

- We can then use the procedure on the previous page to generate the required samples. Specifically,
  + For each $i=1,\ldots,m$, generate $\theta_i \sim f$. If $\theta_i \in [a,b]$, accept $\theta_i$, otherwise **reject** and try again. 
  + Easy to show that this is equivalent to accepting each $\theta_i$ with probability $w(\theta)/M$.



---
## Example

```{r}
#Simple code for using rejection sampling to generate m samples
#from the Beta[10,10] density truncated to (0.35,0.6).
set.seed(12345)
#NOTE: there are more efficient ways to write this code!

#set sample size and reate vector to store sample
m <- 10000; THETA <- rep(0,m)
#keep track of rejects
TotalRejects <- 0; Rejections <- NULL
#now the 'for loop'
for(i in 1:m){
  t <- 0
  while(t < 1){
    theta <- rbeta(1,10,10)
    if(theta > 0.35 & theta < 0.6){
      THETA[i] <- theta
      t <- 1
    } else {
    TotalRejects <- TotalRejects + 1
    Rejections <- rbind(Rejections,theta)
  }
}
}
#How many rejections in all, to generate m samples?
TotalRejects
```

Clearly less efficient than inverse-CDF method which we already know we can use for this exercise. Acceptance rate $\approx 0.726$.


---
## Example

How does our sample compare to the true truncated density?
```{r fig.height=5, echo=F}
p <- seq(0,1,length=10000)
f1 <- dbeta(p,10,10)*as.numeric(p>0.35 & p<0.6)/ (pbeta(0.6,10,10) - pbeta(0.3,10,10) )
plot(p,f1,type='l',col='green4',xlim=c(0,1),ylab='Density', xlab=expression(theta),
     ylim=c(0,6),lty=1.5)
lines(density(THETA),type='l',col='blue4',lty=1.5)
points(c(Rejections),rep(0,TotalRejects),type='p',col="red2",pch=4)
labels <- c("True Density", "Accepted Samples","Rejects")
legend("topright", inset=.05, labels, lwd=c(2,2,2), lty=c(1.5,1.5,NA),pch=c(NA,NA,4),
       col=c('green4','blue4',"red2"))
```



---
## Importance sampling

- .hlight[Importance sampling] is actually one of the first steps into Monte Carlo analysis, in which simulated values from one distribution are used to explore another.

--

- Simulation from the "wrong distribution" can be incredibly useful as we have seen with rejection sampling and will also see later in this course.

--

- Not used as often anymore but still of practical interest in
  + fairly small problems, in terms of dimension, 
  
--
  + in which the density of the distribution of interest can be easily evaluated, but when it is difficult to sample from directly, and
  
--
  + when it is relatively easy to identify and simulate from distributions that approximate the distribution of interest.
  
--

- Importance sampling and Rejection sampling use the same importance ratio ideas, but the latter leads to exact corrections and so exact samples from $p(\theta)$.



---
## Importance sampling

- Interest lies in expectations of the form (instead of the actual samples)
.block[
.small[
$$H = \int h(\theta) p(\theta) d\theta,$$
]
]

- Write
.block[
.small[
$$H = \int h(\theta) w(\theta) g(\theta) d\theta \ \ \ \textrm{with} \ \ \ w(\theta) = p(\theta)/g(\theta)$$
]
]

  that is, $\mathbb{E}[h(\theta)]$ under $p(\theta)$ is just $\mathbb{E}[h(\theta) w(\theta)]$ under $g(\theta)$.
  
- Using direct Monte Carlo integration
.block[
.small[
$$\bar{h} = \dfrac{1}{m} \sum_{i=1}^m w(\theta_i) h(\theta_i).$$
]
]

  where $\theta_1, \ldots, \theta_m \overset{ind}{\sim} g(\theta)$. We are sampling from the "wrong" distribution.



---
## Importance sampling

- The measure of "how wrong" we are at each simulated $\theta_m$ value is the .hlight[importance weight]
.block[
.small[
$$w(\theta_i) = p(\theta_i)/g(\theta_i).$$
]
]

  These ratios weight the sample estimates $h(\theta_i)$ to "correct" for the fact that we sampled the wrong distribution.
  
- See [Lopes & Gamerman (Ch 3.4)](https://www.amazon.com/Markov-Chain-Monte-Carlo-Statistical/dp/1584885874) and [Robert and Casella (Ch. 3.3)](https://www.amazon.com/Monte-Statistical-Methods-Springer-Statistics/dp/1441919392) for discussion of convergence and optimality.

--

- Clearly, the closer $g$ is to $p$, the better the results, just as we had with rejection sampling.



---
## Importance sampling

- Key considerations:
  + MC estimate $\bar{h}$ has the expectation $H$; and is generally almost surely convergent to $H$ (under certain conditions of course but we will not dive into those).
  
--
  + $\mathbb{V}[\bar{h}]$ is often going to be finite in cases in which, generally, $w(\theta) = p(\theta)/g(\theta)$ is bounded and decays rapidly in the tails of $p(\theta)$.
  
--
  + Thus, superior MC approximations, are achieved for choices of $g(\theta)$ whose tails dominate those of the target $p(\theta)$.
  
--
  + That is, importance sampling distributions should be chosen to have tails at least as fat as the target (think normal distribution vs t-distribution).
  
--
  + Obviously require the support of $g(\theta)$ to be the same as, or contain, that of $p(\theta)$.
  
--

- These also clearly apply to rejection sampling too.
  
  
  
---
## Importance sampling

- Problems in which $w(\theta) = p(\theta)/g(\theta)$ can be computed are actually rare.

--

- As you will see when we move away from conjugate distributions, we usually only know $p(\theta)$ up to a normalizing constant.

--

- When this is the case, simply "re-normalize" the importance weights, so that
.block[
.small[
$$\bar{h} = \dfrac{1}{m} \sum_{i=1}^m w_i h(\theta_i) \ \ \ \textrm{where} \ \ \ w_i = \dfrac{w(\theta_i)}{\sum^m_{i=1}w(\theta_i)}.$$
]
]

--

- Generally, in importance sampling, weights that are close to uniform are desirable, and very unevenly distributed weights are not.



---
## Proof for simple accept/reject

- We need to show that all accepted $\theta$ values are indeed from $p(\theta)$. Equivalently, show that $f(\theta | u < w(\theta)/M) = p(\theta)$.

--

- By Bayes' theorem,
.block[
.small[
$$f(\theta | u < w(\theta)/M) = \dfrac{\Pr(\theta \textrm{ and } u < w(\theta)/M)}{\Pr(u < w(\theta)/M)} = \dfrac{\Pr(u < w(\theta)/M \ | \ \theta) g(\theta)}{\Pr(u < w(\theta)/M)}.$$
]
]

--

- But,
  + $\Pr(u < w(\theta)/M \ | \ \theta) = w(\theta)/M$ since $u \sim U(0,1)$, and
  
--
  + 
.block[
.small[
$$
\begin{split}
\Pr(u < w(\theta)/M) & = \int \Pr(u < w(\theta)/M \ | \ \theta) g(\theta) d\theta\\
& = \int w(\theta)/M g(\theta) d\theta = 1/M \int w(\theta) g(\theta) d\theta = 1/M \int p(\theta) d\theta = 1/M.
\end{split}
$$
]
]

--

- Therefore,
.block[
.small[
$$f(\theta | u < w(\theta)/M) = \dfrac{\Pr(u < w(\theta)/M \ | \ \theta) g(\theta)}{\Pr(u < w(\theta)/M)} = \dfrac{w(\theta)/M g(\theta)}{1/M} = w(\theta) g(\theta) = p(\theta).$$
]
]




