---
title: "Poisson model (wrap-up); Monte Carlo approximation and sampling"
author: "Dr. Olanrewaju Michael Akande"
date: "Jan 24, 2020"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
```


## Announcements

- Add/drop today

- HW1 due tomorrow

- Take the participation quiz for today on Sakai
  

---
## Outline

- Loss functions and Bayes risk

- Frequentist vs Bayesian intervals

- Poisson-Gamma model
  + Recap of the distributions
  + Conjugacy
  + Example
  + Posterior prediction
  + Other parameterizations
  


---
class: center, middle

# Loss functions and Bayes risk


---
## Poisson model (wrap-up)

- Generally, it turns out that if 
  + $f(y_i; \theta): y_1,\ldots,y_n \overset{iid}{\sim} \textrm{Po}(\theta)$, and
  + $\pi(\theta): \theta \sim \textrm{Ga}(a,b)$,
  
  then the posterior distribution is also a gamma distribution.
  
--

- <div class="question">
Can we derive the posterior distribution and its parameters? Of course....let's do some work on the board!
</div>

--

- Updating a gamma prior with a Poisson likelihood leads to a gamma posterior - we once again have conjugacy!

--

- Specifically, we have.
.block[
.small[
$$\pi(\theta | \{y_i\}): \theta | \{y_i\} \sim \textrm{Ga}(a+\sum y_i,b+n).$$
]
]

--

- <div class="question">
What is the posterior mean? How about the posterior variance?
</div>



---
## Hoff example: birth rates

- Survey data on educational attainment and number of children of 155 forty-year-old women during the 1990's.

--

- These women were in their 20s during the 1970s, a period of historically low fertility rates in the US.

--

- **Goal**: compare birth rate $\theta_1$ for women with bachelor's degrees to the rate $\theta_2$ for women without.

--

- **Data**:
  + 111 women without a bachelor's degree had 217 children: $(\bar{y}_1 = 1.95)$
  + 44 women with bachelor's degrees had 66 children: $(\bar{y}_2 = 1.50)$
  
--

- Based on the data alone, looks like $\theta_1$ should be greater than $\theta_2$. But...how sure are we?
  
--

- **Priors**: $\theta_1, \theta_2 \sim \textrm{Ga}(2,1)$ (not much prior information; equivalent to 1 prior woman with 2 children). Posterior means will be close to the MLEs.



---
## Hoff example: birth rates

- Then,
  + $\theta_1 | \{n_1=111, \sum y_{i,1}=217\} \sim \textrm{Ga}(2+217,1+111) = \textrm{Ga}(219,112).$
  
  + $\theta_1 | \{n_1=44, \sum y_{i,1}=66\} \sim \textrm{Ga}(2+66,1+44) = \textrm{Ga}(68,45).$
  
--

- We can then use R to calculate posterior means, modes, and 95% credible intervals for $\theta_1$ and $\theta_2$.
    ```{r eval=F}
a=2; b=1; #prior
n1=111; sumy1=217; n2=44; sumy2=66 #data
(a+sumy1)/(b+n1); (a+sumy2)/(b+n2); #post means
qgamma(c(0.025, 0.975),a+sumy1,b+n1) #95\% ci 1
qgamma(c(0.025, 0.975),a+sumy2,b+n2) #95\% ci 2
```

--

- Posterior means: $\mathbb{E}[\theta_1 | \{y_{i,1}\}] = 1.955$ and $\mathbb{E}[\theta_2 | \{y_{i,2}\}] = 1.511$.
  
--

- 95% credible intervals
  + $\theta_1$: [1.71, 2.22].
  + $\theta_2$: [1.17, 1.89].
  


---
## Hoff example: birth rates

Prior and posteriors:

```{r fig.height=5, echo=F}
x <- seq(0,6,by=0.00001)
plot(x,dgamma(x,shape=2,rate=1),xlim=c(0,6),ylim=c(0,3),lwd=1.5,type="l",
      col="green3",xlab=expression(theta),ylab="density")
lines(x,dgamma(x,shape=219,rate=112),col="red2",lwd=1.5,type="l")
lines(x,dgamma(x,shape=68,rate=45),col="blue2",lwd=1.5,type="l")
legend("topright", legend=c("Prior","Post. (No Bach)","Post. (Bach)"),
       col=c("green3","red2","blue2"), lwd=2, cex=1)
```




---
## Hoff example: birth rates

- Posteriors indicate considerable evidence birth rates are higher among women without bachelor's degrees.

--

- Confirms what we observed.

--

- Using sampling we can quickly calculate $\Pr(\theta_1 > \theta_2 | \textrm{data})$.
    ```{r eval=F}
mean(rgamma(10000,219,112)>rgamma(10000,68,45))
```

  We have $\Pr(\theta_1 > \theta_2 | \textrm{data}) = 0.97$.
  
--

- Why/how does it work?

--

- .hlight[Monte Carlo approximation] coming soon!

--

- Clearly, that probability will change with different priors.



---
## Posterior predictive distribution

- <div class="question">
What is the posterior predictive distribution for the Poisson-gamma model?
</div>

--

- Let $a_n = a+\sum y_i$ and $b_n = b+n$.

--

- We have
.block[
.small[
$$
\begin{aligned}
f(y_{n+1}|y_{1:n}) &= \int f(y_{n+1}|\theta) \pi(\theta|y_{1:n})\,d\theta \\
&= \int \textrm{Po}(y_{n+1}; \theta)  \textrm{Ga}(\theta; a_n,b_n)\,d\theta \\
&= ... \\
&= ... \\
&= \dfrac{\Gamma(a_n + y_{n+1})}{\Gamma(a_n)\Gamma(y_{n+1}+1)} \ \left(\dfrac{b_n}{b_n + 1} \right)^{a_n} \ \left(\dfrac{1}{b_n + 1} \right)^{y_{n+1}}
\end{aligned}
$$
]
]

  which is the .hlight[negative binomial distribution], $\textrm{Neg-binomial}\left(a_n,\dfrac{1}{b_n + 1}\right)$.
  
--

- The .hlight[prior predictive distribution] takes a similar form.



---
## Negative binomial distribution

- Originally derived as the number of successes in a sequence of independent $\textrm{Bernoulli}(p)$ trials before $r$ failures occur.

--

- The negative binomial distribution $\textrm{Neg-binomial}\left(r,p\right)$ is parameterized by $r$ and $p$ and the pmf is given by
.block[
.small[
$$\Pr[Y = y | r, p] = {y + r - 1 \choose y} (1-p)^rp^y; \ \ \ \ y=0,1,2,\ldots; \ \ \ p \in [0,1].$$
]
]

- Starting with this, the distribution can be extended to allow $r \in (0,\infty)$ as
.block[
.small[
$$\Pr[Y = y | r, p] = \dfrac{\Gamma(y+r)}{\Gamma(y+1)\Gamma(r)} (1-p)^rp^y; \ \ \ \ y=0,1,2,\ldots; \ \ \ p \in [0,1].$$
]
]

--

- Some properties:

  + $\mathbb{E}[\theta] = \dfrac{pr}{1-p}$
  
  + $\mathbb{V}[\theta] = \dfrac{pr}{(1-p)^2}$




---
## Posterior predictive distribution

- The negative binomial distribution is an over-dispersed generalization of the Poisson.

--

- <div class="question">
What does over-dispersion mean?
</div>

--

- In marginalizing $\theta$ out of the Poisson likelihood, over a gamma distribution, we obtain a negative-binomial.

--

- For $(y_{n+1}|y_{1:n}) \sim \textrm{Neg-binomial}\left(a_n,\dfrac{1}{b_n + 1}\right)$, we have

  + $\mathbb{E}[y_{n+1}|y_{1:n}] = \dfrac{a_n}{b_n} = \mathbb{E}[\theta|y_{1:n}] =$ posterior mean, and
  
  + $\mathbb{V}[y_{n+1}|y_{1:n}] = \dfrac{a_n(b_n+1)}{b_n^2} = \mathbb{E}[\theta|y_{1:n}] \left(\dfrac{b_n+1}{b_n}\right)$,
  
    so that variance is larger than the mean by an amount determined by $b_n$, which takes the over-dispersion into account.



---
## Predictive uncertainty

- Note that as the sample size $n$ increases, the posterior density for $\theta$ becomes more and more concentrated.
.block[
.small[
$$\mathbb{E}[\theta | y_{1:n}] = \dfrac{a_n}{b_n^2} = \dfrac{a + \sum_i y_i}{(b + n)^2} \approx \dfrac{\bar{y}}{n} \rightarrow 0.$$
]
]

--

- Also, recall that $\mathbb{V}[y_{n+1}|y_{1:n}] = \mathbb{E}[\theta] \left(\dfrac{b_n+1}{b_n}\right)$.

--

- As we have less uncertainty about $\theta$, the inflation factor
.block[
.small[
$$\dfrac{b_n+1}{b_n} = \dfrac{b + n+1}{b + n} \rightarrow 1$$
]
]

  and the predictive density $f(y_{n+1}|y_{1:n}) \rightarrow \textrm{Po}(\bar{y})$.
  
--

- Of course, in smaller samples, it is important to inflate our predictive intervals to account for uncertainty in $\theta$.



---
## Back to birth rates

- Let's compare the posterior predictive distributions for the two groups of women.
```{r fig.height=4.9, echo=F}
a=2; b=1; #prior
n1=111; sumy1=217; n2=44; sumy2=66 #data
y=0:10
ltbach=dnbinom(y,size=(a+sumy1),mu=(a+sumy1)/(b+n1))
bach=dnbinom(y,size=(a+sumy2),mu=(a+sumy2)/(b+n2))
barplot(rbind(ltbach,bach),xlab="Children", 
        names.arg=c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
        col=c("orange3","green3"),beside=TRUE, main="Posterior predictive distributions")
legend("topright",c("No Bachelors","Bachelors"),bty="n", fill=c("orange3","green3"))
```



---
## Back to birth rates

- Suppose we randomly sample two women, one with degree and one without. To what extent do we expect the one without the degree to have more kids than the other, e.g. $\tilde{y}_1 > \tilde{y}_2$?

--

- Using R, $\Pr(\tilde{y}_1 > \tilde{y}_2) \approx 0.48$ and $\Pr(\tilde{y}_1 > \tilde{y}_2) \approx 0.22$.
    ```{r fig.height=4}
    set.seed(01222020)
a=2; b=1; #prior
n1=111; sumy1=217; n2=44; sumy2=66 #data
mean(rnbinom(100000,size=(a+sumy1),mu=(a+sumy1)/(b+n1)) >
rnbinom(10000,size=(a+sumy2),mu=(a+sumy2)/(b+n2)))
mean(rnbinom(100000,size=(a+sumy1),mu=(a+sumy1)/(b+n1))==
rnbinom(10000,size=(a+sumy2),mu=(a+sumy2)/(b+n2)))
```

--

- Strong evidence of difference between two populations does not really imply the difference in predictions is large.


---
## Poisson model in terms of rate

- In many applications, it is often convenient to parameterize the Poisson model a bit differently. One option takes the form
.block[
.small[
$$y_1,\ldots,y_n \sim \textrm{Po}(x_i\theta)$$
]
]

  where $x_i$ represents an explanatory variable and $\theta$ is once again the population parameter of interest. The model is not exchangeable in the $y_i$'s but is exchangeable in the pairs $(x,y)_i$.
  
--

- In epidemiology, $\theta$ is often called the population "rate" and $x_i$ is called the "exposure" of unit $i$.

--

- When dealing with mortality rates in different counties for example, $x_i$ can be the population $n_i$ in county $i$, with $\theta =$ the overall mortality rate.

--

- The gamma distribution is still conjugate for $\theta$, with the resulting posterior taking the form
.block[
.small[
$$\pi(\theta | \{x_i, y_i\}): \theta | \{x_i, y_i\} \sim \textrm{Ga}(a+\sum_i y_i,b+\sum_i x_i).$$
]
]

--

- We will look at an example in the next class.




