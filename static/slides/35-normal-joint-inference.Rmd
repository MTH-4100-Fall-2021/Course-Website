---
title: "STA 360/602L: Module 3.5"
subtitle: "The normal model: joint inference for mean and variance"
author: "Dr. Olanrewaju Michael Akande"
date: " "
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
```



## Joint inference for mean and variance

- What happens when $\sigma$/ $\tau$ is unknown? We need a joint prior $\pi(\mu,\sigma^2)$ for $\mu$ and $\sigma^2$.

--

- Write the joint prior distribution for the mean and variance as the product of a conditional and a marginal distribution, so we can take advantage of our work so far.

--

- That is,
.block[
.small[
$$\pi(\mu,\sigma^2) \ = \  \pi(\mu | \sigma^2)\pi(\sigma^2).$$
]
]

--

- For $\pi(\sigma^2)$, we need a distribution with support on $(0,\infty)$. One such family is the gamma family, but this is NOT conjugate for the variance of a normal distribution.

--

- The gamma distribution is, however, conjugate for the precision $\tau$, and in that case, we say that $\sigma^2$ has an .hlight[inverse-gamma] distribution.



---
## Conditional specification of prior

- Once again, suppose $Y = (y_1,y_2,\ldots,y_n)$, where each
.block[
.small[
$$y_i \sim \mathcal{N}(\mu, \sigma^2); \ \ \ \textrm{or} \ \ \ y_i \sim \mathcal{N}(\mu, \tau^{-1}).$$
]
]

--

- A conjugate joint prior is given by
.block[
.small[
$$
\begin{split}
\tau = \dfrac{1}{\sigma^2} \ & \sim \textrm{Gamma}\left(\dfrac{\nu_0}{2}, \dfrac{\nu_0\sigma_0^2}{2}\right)\\
\mu|\sigma^2 & \sim \mathcal{N}\left(\mu_0, \dfrac{\sigma^2}{\kappa_0}\right) \ \ \ \textrm{or} \ \ \ \mu|\tau & \sim \mathcal{N}\left(\mu_0, \dfrac{1}{\kappa_0 \tau}\right).\\
\end{split}
$$
]
]

--

- This is often called a .hlight[normal-gamma] prior distribution.

--

- $\sigma_0^2$ is the prior guess for $\sigma^2$, while $\nu_0$ is often referred to as the "prior degrees of freedom", our degree of confidence in $\sigma_0^2$.

--

- We do not have conjugacy if we replace $\dfrac{\sigma^2}{\kappa_0}$ in the normal prior with an arbitrary prior variance independent of $\sigma^2$. To do inference in that scenario, we need .hlight[Gibbs sampling] (to come next week!).



---
## Posterior for the mean given variance, under normal-gamma prior

- Based on the normal-gamma prior, we need $\pi(\mu|Y,\sigma^2)$ and $\pi(\tau|Y)$.

--

- For $\pi(\mu|Y,\sigma^2)$, we can leverage our previous results. We have
.block[
.large[
$$\mu|Y,\sigma^2 \sim \mathcal{N}(\mu_n, \dfrac{\sigma^2}{\kappa_n}) \ \ \ \textrm{or} \ \ \ \mu|Y,\tau \sim \mathcal{N}(\mu_n, \dfrac{1}{\kappa_n \tau})$$
]
]

  where
.block[
.small[
$$\mu_n = \dfrac{ \dfrac{n}{\sigma^2}\bar{y} + \dfrac{\kappa_0}{\sigma^2} \mu_0}{\dfrac{n}{\sigma^2} + \dfrac{\kappa_0}{\sigma^2}} = \dfrac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} = \dfrac{\kappa_0}{\kappa_n} \mu_0 + \dfrac{n}{\kappa_n} \bar{y} \ \ \ \textrm{and} \ \ \ \kappa_n = \kappa_0 + n.$$
]
]

--

- $\mu_n$ is simply the sample mean of the current and prior observations, and posterior variance of $\mu$ given $\sigma^2$ is $\sigma^2$ divided by the total number of observations (prior and current).



---
## Posterior derivation

- Some algebra is required to get the marginal posterior of $\tau$. Let's write the full joint posterior and go from there. We must keep some of the terms we discarded in the last lecture.

--

- Recall the likelihood
.block[
.large[
$$L(Y; \mu,\tau) \propto \tau^{\dfrac{n}{2}} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau s^2(n-1) \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu - \bar{y})^2 \right\},$$
]
]

--

- Now, $\mu|\tau \sim \mathcal{N}\left(\mu_0, \dfrac{1}{\kappa_0 \tau}\right)$ $\Rightarrow$
.block[
.small[
$$\pi(\mu|\tau) \propto \ \textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu - \mu_0)^2 \right\}.$$
]
]

--

- and $\tau \sim \textrm{Ga}\left(\dfrac{\nu_0}{2}, \dfrac{\nu_0\sigma_0^2}{2}\right)$ $\Rightarrow$
.block[
.small[
$$\pi(\tau) \propto \tau^{\dfrac{\nu_0}{2}-1} \textrm{exp}\left\{-\dfrac{\tau\nu_0\sigma_0^2}{2}\right\}.$$
]
]


---
## Posterior derivation

.block[
.small[
$$
\begin{split}
\Rightarrow \pi(\mu,\tau | Y) \ & \boldsymbol{ \propto} \ \pi(\mu|\sigma^2) \times \pi(\tau) \times L(Y; \mu,\sigma^2)\\
 \\
& \boldsymbol{ \propto} \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu - \mu_0)^2 \right\}}_{\propto \ \pi(\mu|\sigma^2)} \times \underbrace{\tau^{\dfrac{\nu_0}{2}-1} \textrm{exp}\left\{-\dfrac{\tau\nu_0\sigma_0^2}{2}\right\}}_{\propto \ \pi(\tau)}\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{n}{2}} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau s^2(n-1) \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu - \bar{y})^2 \right\}}_{\propto \ L(Y; \mu,\sigma^2)}\\
& \boldsymbol{=}  \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu - \mu_0)^2 \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu - \bar{y})^2 \right\}}_{\textrm{Terms involving } \mu}\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{\nu_0}{2}-1} \textrm{exp}\left\{-\dfrac{\tau\nu_0\sigma_0^2}{2}\right\} \tau^{\dfrac{n}{2}} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau s^2(n-1) \right\}}_{\textrm{Terms NOT involving } \mu}\\
& \boldsymbol{=}  \textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu^2 - 2\mu\mu_0 + \mu_0^2) \right\}\  \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu^2 - 2\mu\bar{y} + \bar{y}^2) \right\}\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}
$$
]
]




---
## Posterior derivation

.block[
.small[
$$
\begin{split}
\pi(\mu,\tau | Y) \ & \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} \left[\kappa_0 \tau (\mu^2 - 2\mu\mu_0)  + \tau n(\mu^2 - 2\mu\bar{y}) \right] \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
 \\
& \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \mu^2(n\tau + \kappa_0 \tau) - 2\mu(n\tau \bar{y} + \kappa_0 \tau\mu_0)   \right] \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}
$$
]
]

--

- Set $a^\star = (n\tau + \kappa_0 \tau)$ and $b^\star = (n\tau \bar{y} + \kappa_0 \tau\mu_0)$, then complete the square for the first part like we did in the last lecture.
.block[
.small[
$$
\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ & \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \mu^2 a^\star - 2\mu b^\star   \right] \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}
$$
]
]



---
## Posterior derivation

.block[
.small[
$$
\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ & \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 + \dfrac{(b^\star)^2}{2a^\star} \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
& \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2 - \dfrac{(b^\star)^2}{a^\star}\right] \right\}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
& \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp} \underbrace{\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2 - \dfrac{(n\tau \bar{y} + \kappa_0 \tau\mu_0)^2}{(n\tau + \kappa_0 \tau)}\right] \right\}}_{\textrm{Expand terms and recombine}} \\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
& \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[\dfrac{n\kappa_0\tau^2 (\mu_0^2- 2\mu_0\bar{y} + \bar{y}^2)}{\tau(n + \kappa_0)}\right] \right\}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}
$$ 
]
]



---
## Posterior derivation

.block[
.small[
$$
\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ & \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\dfrac{n\kappa_0 (\bar{y} - \mu_0)^2}{(n + \kappa_0)}\right] \right\}\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
 \\
& \boldsymbol{=} \ \ \textrm{exp} \underbrace{\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\}}_{\textrm{Substitute the values for } a^\star \textrm{ and } b^\star \textrm{ back}} \\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\} \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\dfrac{n\kappa_0 (\bar{y} - \mu_0)^2}{(n + \kappa_0)}\right] \right\}\\
 \\
& \boldsymbol{=} \ \ \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} (n\tau + \kappa_0 \tau) \left[ \mu^2- \dfrac{(n\tau \bar{y} + \kappa_0 \tau\mu_0)}{(n\tau + \kappa_0 \tau)} \right]^2 \right\}}_{\textrm{Normal Kernel}}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\nu_0\sigma_0^2 + s^2(n-1) + \dfrac{n\kappa_0}{(n + \kappa_0)} (\bar{y} - \mu_0)^2 \right] \right\}}_{\textrm{Gamma Kernel}}\\
\end{split}
$$
]
]



---
## Posterior derivation

.block[
.small[
$$
\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ & \boldsymbol{ \propto} \ \ \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} \tau(n + \kappa_0) \left[ \mu^2- \dfrac{(n \bar{y} + \kappa_0 \mu_0)}{(n + \kappa_0)} \right]^2 \right\}}_{\textrm{Normal Kernel}}\\\
& \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\nu_0\sigma_0^2 + s^2(n-1) + \dfrac{n\kappa_0}{(n + \kappa_0)} (\bar{y} - \mu_0)^2 \right] \right\}}_{\textrm{Gamma Kernel}}\\
& \boldsymbol{=} \ \ \mathcal{N}\left(\mu_n, \dfrac{1}{\kappa_n\tau} \right) \times \textrm{Gamma}\left(\dfrac{\nu_n}{2}, \dfrac{\nu_n \sigma_n^2}{2}\right) = \pi(\mu|Y, \tau) \pi(\tau|Y),
\end{split}
$$
]
]

   where
.block[
.small[
$$
\begin{split}
\kappa_n & = \kappa_0 + n\\
\mu_n & = \dfrac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} = \dfrac{\kappa_0}{\kappa_n} \mu_0 + \dfrac{n}{\kappa_n} \bar{y}\\
\nu_n & = \nu_0 + n\\
\sigma_n^2 & = \dfrac{1}{\nu_n}\left[\nu_0\sigma_0^2 + s^2(n-1) + \dfrac{n\kappa_0}{\kappa_n} (\bar{y} - \mu_0)^2 \right] = \dfrac{1}{\nu_n}\left[\nu_0\sigma_0^2 + \sum_{i=1}^n (y_i-\bar{y})^2 + \dfrac{n\kappa_0}{\kappa_n} (\bar{y} - \mu_0)^2 \right]\\
\end{split}
$$
]
]

--

- Turns out that the marginal posterior of $\mu$, that is, $\pi(\mu|Y) = \int_0^\infty \pi(\mu, \tau|Y) d\tau$ is a **t-distribution**. You can derive that distribution if you are interested, we won't spend time on it in class. 



---
## Back to our examples

- .hlight[Pygmalion: questions of interest]
  + Is the average improvement for the accelerated group larger than that for the no growth group?
      - What is $\Pr[\mu_A > \mu_N | Y_A, Y_N)$?
  + Is the variance of improvement scores for the accelerated group larger than that for the no growth group?
      - What is $\Pr[\sigma^2_A > \sigma^2_N | Y_A, Y_N)$?
      
--

- .hlight[Job training: questions of interest]
  + Is the average change in annual earnings for the training group larger than that for the no training group?
      - What is $\Pr[\mu_T > \mu_N | Y_T, Y_N)$?
  + Is the variance of change in annual earnings for the training group larger than that for the no training group?
      - What is $\Pr[\sigma^2_T > \sigma^2_N | Y_T, Y_N)$?



---
## Mildly informative priors

- We will focus on the Pygmalion study. Follow the same approach for the job training data.

--

- Suppose you have no idea whether students would improve IQ on average. Set $\mu_{0A} = \mu_{0N} = 0$.

--

- Suppose you don't have any faith in this belief, and think it is the equivalent of having only 1 prior observation in each group. Set $\kappa_{0A} = \kappa_{0N} = 1$.

--

- Based on the literature, SD of change scores should be around 10 in each group, but still you don't have a lot of faith in this belief. Set 
$\nu_{0A} = \nu_{0N} = 1$ and $\sigma^2_{0A} = \sigma^2_{0N} = 100$.

--

- Graph priors to see if they accord with your beliefs. Sampling new values of $Y$ from the priors offers a good check.



---
## Recall the Pygmalion data

- Data:
  
  + Accelerated group (A): 20, 10, 19, 15, 9, 18.
  
  + No growth group (N): 3, 2, 6, 10, 11, 5.
--

- Summary statistics:
  
  + $\bar{y}_A = 15.2$; $s_A = 4.71$.
  
  + $\bar{y}_N = 6.2$; $s_N = 3.65$.



---
## Analysis with mildly informative priors

.block[
.small[
$$
\begin{split}
\kappa_{nA} & = \kappa_{0A} + n_A = 1 + 6 = 7\\
\kappa_{nN} & = \kappa_{0N} + n_N = 1 + 6 = 7\\
\nu_{nA} & = \nu_{0A} + n_A = 1 + 6 = 7\\
\nu_{nN} & = \nu_{0N} + n_N = 1 + 6 = 7\\
\mu_{nA} & = \dfrac{\kappa_{0A} \mu_{0A} + n_A\bar{y}_A}{\kappa_{nA}} = \dfrac{ (1)(0) + (6)(15.2) }{7} \approx 13.03\\
\mu_{nN} & = \dfrac{\kappa_{0N} \mu_{0N} + n_N\bar{y}_N}{\kappa_{nN}} = \dfrac{ (1)(0) + (6)(6.2) }{7} \approx 5.31\\
\sigma_{nA}^2 & = \dfrac{1}{\nu_{nA}}\left[\nu_{0A}\sigma_{0A}^2 + s^2_A(n_A-1) + \dfrac{n_A\kappa_{0A}}{\kappa_{nA}} (\bar{y}_A - \mu_{0A})^2\right] \\
& = \dfrac{1}{7}\left[(1)(100) + (22.17)(5) + \dfrac{(6)(1)}{(7)} (15.2- 0)^2\right] \approx 58.41\\
\sigma_{nN}^2 & = \dfrac{1}{\nu_{nN}}\left[\nu_{0N}\sigma_{0N}^2 + s^2_N(n_N-1) + \dfrac{n_N\kappa_{0N}}{\kappa_{nN}} (\bar{y}_N - \mu_{0N})^2\right] \\
& = \dfrac{1}{7}\left[(1)(100) + (13.37)(5) + \dfrac{(6)(1)}{(7)} (6.2- 0)^2\right] \approx 28.54\\
\end{split}
$$
]
]



---
## Analysis with mildly informative priors

- So our joint posterior is
.block[
.small[
$$
\begin{split}
\mu_A | Y_A, \tau_A & \sim \ \mathcal{N}\left(\mu_{nA}, \dfrac{1}{\kappa_{nA}\tau_A} \right) = \mathcal{N}\left(13.03, \dfrac{1}{7\tau_A} \right)\\
\tau_A | Y_A & \sim \textrm{Gamma}\left(\dfrac{\nu_{nA}}{2}, \dfrac{\nu_{nA} \sigma_{nA}^2}{2}\right) = \textrm{Gamma}\left(\dfrac{7}{2}, \dfrac{7 (58.41)}{2}\right)\\
\mu_N | Y_N, \tau_N & \sim \ \mathcal{N}\left(\mu_{nN}, \dfrac{1}{\kappa_{nN}\tau_N} \right) = \mathcal{N}\left(5.31, \dfrac{1}{7\tau_N} \right)\\
\tau_N | Y_N & \sim \textrm{Gamma}\left(\dfrac{\nu_{nN}}{2}, \dfrac{\nu_{nN} \sigma_{nN}^2}{2}\right) = \textrm{Gamma}\left(\dfrac{7}{2}, \dfrac{7 (28.54)}{2}\right)\\
\end{split}
$$
]
]



---
## Monte Carlo sampling

- To evaluate whether the accelerated group has larger IQ gains than the normal group, we would like to estimate quantities like $\Pr[\mu_A > \mu_N | Y_A, Y_N)$ which are based on the **marginal posterior** of $\mu$ rather than the **conditional distribution**.

--

- Fortunately, this is easy to do by generating samples of $\mu$ and $\sigma^2$ from their joint posterior.



---
## Monte Carlo sampling

- Suppose we simulate values using the following Monte Carlo procedure:
.block[
.small[
$$
\begin{split}
\tau^{(1)} & \sim \textrm{Gamma}\left(\dfrac{\nu_{n}}{2}, \dfrac{\nu_{n} \sigma_{n}^2}{2}\right)\\
\mu^{(1)} & \sim \ \mathcal{N}\left(\mu_{n}, \dfrac{1}{\kappa_{n}\tau^{(1)}} \right)\\
\tau^{(2)} & \sim \textrm{Gamma}\left(\dfrac{\nu_{n}}{2}, \dfrac{\nu_{n} \sigma_{n}^2}{2}\right)\\
\mu^{(2)} & \sim \ \mathcal{N}\left(\mu_{n}, \dfrac{1}{\kappa_{n}\tau^{(2)}} \right)\\
& \ \ \vdots \\
& \ \ \vdots \\
& \ \ \vdots \\
\tau^{(m)} & \sim \textrm{Gamma}\left(\dfrac{\nu_{n}}{2}, \dfrac{\nu_{n} \sigma_{n}^2}{2}\right)\\
\mu^{(m)} & \sim \ \mathcal{N}\left(\mu_{n}, \dfrac{1}{\kappa_{n}\tau^{(m)}} \right)\\
\end{split}
$$
]
]


---
## Monte Carlo sampling

- Note that we are sampling each $\mu^{(j)}$, $j=1,\ldots,m$, from its conditional distribution, not from the marginal.

--

- The sequence of pairs $\{(\tau, \mu)^{(1)}, \ldots, (\tau, \mu)^{(m)}\}$ simulated using this method are independent samples from the joint posterior $\pi(\mu,\tau | Y)$.

--

- Additionally, the simulated sequence $\{\mu^{(1)}, \ldots, \mu^{(m)}\}$ are independent samples from the **marginal posterior distribution**.

--

- While this may seem odd, keep in mind that while we drew the $\mu$'s as conditional samples, each was conditional on a different value of $\tau$.

--

- Thus, together they constitute marginal samples of $\mu$.



---
## Monte Carlo sampling

It is easy to sample from these posteriors:
```{r}
aA <- 7/2
aN <- 7/2
bA <- (7/2)*58.41
bN <- (7/2)*28.54
muA <- 13.03
muN <- 5.31
kappaA <- 7
kappaN <- 7
tauA_postsample <- rgamma(10000,aA,bA)
thetaA_postsample <- rnorm(10000,muA,sqrt(1/(kappaA*tauA_postsample)))
tauN_postsample <- rgamma(10000,aN,bN)
thetaN_postsample <- rnorm(10000,muN,sqrt(1/(kappaN*tauN_postsample)))
sigma2A_postsample <- 1/tauA_postsample
sigma2N_postsample <- 1/tauN_postsample
```



---
## Monte Carlo sampling

- Is the average improvement for the accelerated group larger than that for the no growth group?
  + What is $\Pr[\mu_A > \mu_N | Y_A, Y_N)$?
  ```{r}
mean(thetaA_postsample > thetaN_postsample)
```

--

- Is the variance of improvement scores for the accelerated group larger than that for the no growth group?
  + What is $\Pr[\sigma^2_A > \sigma^2_N | Y_A, Y_N)$?
  ```{r}
mean(sigma2A_postsample > sigma2N_postsample)
```
      
--

- <div class="question">
What can we conclude from this?
</div>



---

class: center, middle

# What's next? 

### Move on to the readings for the next module!




