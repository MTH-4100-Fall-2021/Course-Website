<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STA 360/602L: Module 3.5</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# STA 360/602L: Module 3.5
## The normal model: joint inference for mean and variance
### Dr. Olanrewaju Michael Akande

---








## Joint inference for mean and variance

- What happens when `\(\sigma\)`/ `\(\tau\)` is unknown? We need a joint prior `\(\pi(\mu,\sigma^2)\)` for `\(\mu\)` and `\(\sigma^2\)`.

--

- Write the joint prior distribution for the mean and variance as the product of a conditional and a marginal distribution, so we can take advantage of our work so far.

--

- That is,
.block[
.small[
`$$\pi(\mu,\sigma^2) \ = \  \pi(\mu | \sigma^2)\pi(\sigma^2).$$`
]
]

--

- For `\(\pi(\sigma^2)\)`, we need a distribution with support on `\((0,\infty)\)`. One such family is the gamma family, but this is NOT conjugate for the variance of a normal distribution.

--

- The gamma distribution is, however, conjugate for the precision `\(\tau\)`, and in that case, we say that `\(\sigma^2\)` has an .hlight[inverse-gamma] distribution.



---
## Conditional specification of prior

- Once again, suppose `\(Y = (y_1,y_2,\ldots,y_n)\)`, where each
.block[
.small[
`$$y_i \sim \mathcal{N}(\mu, \sigma^2); \ \ \ \textrm{or} \ \ \ y_i \sim \mathcal{N}(\mu, \tau^{-1}).$$`
]
]

--

- A conjugate joint prior is given by
.block[
.small[
$$
`\begin{split}
\tau = \dfrac{1}{\sigma^2} \ &amp; \sim \textrm{Gamma}\left(\dfrac{\nu_0}{2}, \dfrac{\nu_0\sigma_0^2}{2}\right)\\
\mu|\sigma^2 &amp; \sim \mathcal{N}\left(\mu_0, \dfrac{\sigma^2}{\kappa_0}\right) \ \ \ \textrm{or} \ \ \ \mu|\tau &amp; \sim \mathcal{N}\left(\mu_0, \dfrac{1}{\kappa_0 \tau}\right).\\
\end{split}`
$$
]
]

--

- This is often called a .hlight[normal-gamma] prior distribution.

--

- `\(\sigma_0^2\)` is the prior guess for `\(\sigma^2\)`, while `\(\nu_0\)` is often referred to as the "prior degrees of freedom", our degree of confidence in `\(\sigma_0^2\)`.

--

- We do not have conjugacy if we replace `\(\dfrac{\sigma^2}{\kappa_0}\)` in the normal prior with an arbitrary prior variance independent of `\(\sigma^2\)`. To do inference in that scenario, we need .hlight[Gibbs sampling] (to come next week!).



---
## Posterior for the mean given variance, under normal-gamma prior

- Based on the normal-gamma prior, we need `\(\pi(\mu|Y,\sigma^2)\)` and `\(\pi(\tau|Y)\)`.

--

- For `\(\pi(\mu|Y,\sigma^2)\)`, we can leverage our previous results. We have
.block[
.large[
`$$\mu|Y,\sigma^2 \sim \mathcal{N}(\mu_n, \dfrac{\sigma^2}{\kappa_n}) \ \ \ \textrm{or} \ \ \ \mu|Y,\tau \sim \mathcal{N}(\mu_n, \dfrac{1}{\kappa_n \tau})$$`
]
]

  where
.block[
.small[
`$$\mu_n = \dfrac{ \dfrac{n}{\sigma^2}\bar{y} + \dfrac{\kappa_0}{\sigma^2} \mu_0}{\dfrac{n}{\sigma^2} + \dfrac{\kappa_0}{\sigma^2}} = \dfrac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} = \dfrac{\kappa_0}{\kappa_n} \mu_0 + \dfrac{n}{\kappa_n} \bar{y} \ \ \ \textrm{and} \ \ \ \kappa_n = \kappa_0 + n.$$`
]
]

--

- `\(\mu_n\)` is simply the sample mean of the current and prior observations, and posterior variance of `\(\mu\)` given `\(\sigma^2\)` is `\(\sigma^2\)` divided by the total number of observations (prior and current).



---
## Posterior derivation

- Some algebra is required to get the marginal posterior of `\(\tau\)`. Let's write the full joint posterior and go from there. We must keep some of the terms we discarded in the last lecture.

--

- Recall the likelihood
.block[
.large[
`$$L(Y; \mu,\tau) \propto \tau^{\dfrac{n}{2}} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau s^2(n-1) \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu - \bar{y})^2 \right\},$$`
]
]

--

- Now, `\(\mu|\tau \sim \mathcal{N}\left(\mu_0, \dfrac{1}{\kappa_0 \tau}\right)\)` `\(\Rightarrow\)`
.block[
.small[
`$$\pi(\mu|\tau) \propto \ \textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu - \mu_0)^2 \right\}.$$`
]
]

--

- and `\(\tau \sim \textrm{Ga}\left(\dfrac{\nu_0}{2}, \dfrac{\nu_0\sigma_0^2}{2}\right)\)` `\(\Rightarrow\)`
.block[
.small[
`$$\pi(\tau) \propto \tau^{\dfrac{\nu_0}{2}-1} \textrm{exp}\left\{-\dfrac{\tau\nu_0\sigma_0^2}{2}\right\}.$$`
]
]


---
## Posterior derivation

.block[
.small[
$$
`\begin{split}
\Rightarrow \pi(\mu,\tau | Y) \ &amp; \boldsymbol{ \propto} \ \pi(\mu|\sigma^2) \times \pi(\tau) \times L(Y; \mu,\sigma^2)\\
 \\
&amp; \boldsymbol{ \propto} \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu - \mu_0)^2 \right\}}_{\propto \ \pi(\mu|\sigma^2)} \times \underbrace{\tau^{\dfrac{\nu_0}{2}-1} \textrm{exp}\left\{-\dfrac{\tau\nu_0\sigma_0^2}{2}\right\}}_{\propto \ \pi(\tau)}\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{n}{2}} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau s^2(n-1) \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu - \bar{y})^2 \right\}}_{\propto \ L(Y; \mu,\sigma^2)}\\
&amp; \boldsymbol{=}  \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu - \mu_0)^2 \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu - \bar{y})^2 \right\}}_{\textrm{Terms involving } \mu}\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{\nu_0}{2}-1} \textrm{exp}\left\{-\dfrac{\tau\nu_0\sigma_0^2}{2}\right\} \tau^{\dfrac{n}{2}} \ \textrm{exp}\left\{-\dfrac{1}{2} \tau s^2(n-1) \right\}}_{\textrm{Terms NOT involving } \mu}\\
&amp; \boldsymbol{=}  \textrm{exp}\left\{-\dfrac{1}{2} \kappa_0 \tau (\mu^2 - 2\mu\mu_0 + \mu_0^2) \right\}\  \textrm{exp}\left\{-\dfrac{1}{2} \tau n(\mu^2 - 2\mu\bar{y} + \bar{y}^2) \right\}\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}`
$$
]
]




---
## Posterior derivation

.block[
.small[
$$
`\begin{split}
\pi(\mu,\tau | Y) \ &amp; \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} \left[\kappa_0 \tau (\mu^2 - 2\mu\mu_0)  + \tau n(\mu^2 - 2\mu\bar{y}) \right] \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
 \\
&amp; \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \mu^2(n\tau + \kappa_0 \tau) - 2\mu(n\tau \bar{y} + \kappa_0 \tau\mu_0)   \right] \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}`
$$
]
]

--

- Set `\(a^\star = (n\tau + \kappa_0 \tau)\)` and `\(b^\star = (n\tau \bar{y} + \kappa_0 \tau\mu_0)\)`, then complete the square for the first part like we did in the last lecture.
.block[
.small[
$$
`\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ &amp; \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \mu^2 a^\star - 2\mu b^\star   \right] \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}`
$$
]
]



---
## Posterior derivation

.block[
.small[
$$
`\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ &amp; \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 + \dfrac{(b^\star)^2}{2a^\star} \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2\right] \right\}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
&amp; \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2 - \dfrac{(b^\star)^2}{a^\star}\right] \right\}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
&amp; \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp} \underbrace{\left\{-\dfrac{1}{2} \left[ \kappa_0 \tau \mu_0^2 + \tau n \bar{y}^2 - \dfrac{(n\tau \bar{y} + \kappa_0 \tau\mu_0)^2}{(n\tau + \kappa_0 \tau)}\right] \right\}}_{\textrm{Expand terms and recombine}} \\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
&amp; \boldsymbol{=} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp}\left\{-\dfrac{1}{2} \left[\dfrac{n\kappa_0\tau^2 (\mu_0^2- 2\mu_0\bar{y} + \bar{y}^2)}{\tau(n + \kappa_0)}\right] \right\}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
\end{split}`
$$ 
]
]



---
## Posterior derivation

.block[
.small[
$$
`\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ &amp; \boldsymbol{ \propto} \ \ \textrm{exp}\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\} \ \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\dfrac{n\kappa_0 (\bar{y} - \mu_0)^2}{(n + \kappa_0)}\right] \right\}\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\}\\
 \\
&amp; \boldsymbol{=} \ \ \textrm{exp} \underbrace{\left\{-\dfrac{1}{2} a^\star \left[ \mu- \dfrac{b^\star}{a^\star} \right]^2 \right\}}_{\textrm{Substitute the values for } a^\star \textrm{ and } b^\star \textrm{ back}} \\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau \left[\nu_0\sigma_0^2 + s^2(n-1) \right] }{2}\right\} \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\dfrac{n\kappa_0 (\bar{y} - \mu_0)^2}{(n + \kappa_0)}\right] \right\}\\
 \\
&amp; \boldsymbol{=} \ \ \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} (n\tau + \kappa_0 \tau) \left[ \mu^2- \dfrac{(n\tau \bar{y} + \kappa_0 \tau\mu_0)}{(n\tau + \kappa_0 \tau)} \right]^2 \right\}}_{\textrm{Normal Kernel}}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\nu_0\sigma_0^2 + s^2(n-1) + \dfrac{n\kappa_0}{(n + \kappa_0)} (\bar{y} - \mu_0)^2 \right] \right\}}_{\textrm{Gamma Kernel}}\\
\end{split}`
$$
]
]



---
## Posterior derivation

.block[
.small[
$$
`\begin{split}
\Rightarrow \ \pi(\mu,\tau | Y) \ &amp; \boldsymbol{ \propto} \ \ \underbrace{\textrm{exp}\left\{-\dfrac{1}{2} \tau(n + \kappa_0) \left[ \mu^2- \dfrac{(n \bar{y} + \kappa_0 \mu_0)}{(n + \kappa_0)} \right]^2 \right\}}_{\textrm{Normal Kernel}}\\\
&amp; \ \ \ \ \ \ \ \ \ \ \ \ 
\times \underbrace{\tau^{\dfrac{\nu_0 + n}{2}-1} \textrm{exp}\left\{-\dfrac{\tau}{2} \left[\nu_0\sigma_0^2 + s^2(n-1) + \dfrac{n\kappa_0}{(n + \kappa_0)} (\bar{y} - \mu_0)^2 \right] \right\}}_{\textrm{Gamma Kernel}}\\
&amp; \boldsymbol{=} \ \ \mathcal{N}\left(\mu_n, \dfrac{1}{\kappa_n\tau} \right) \times \textrm{Gamma}\left(\dfrac{\nu_n}{2}, \dfrac{\nu_n \sigma_n^2}{2}\right) = \pi(\mu|Y, \tau) \pi(\tau|Y),
\end{split}`
$$
]
]

   where
.block[
.small[
$$
`\begin{split}
\kappa_n &amp; = \kappa_0 + n\\
\mu_n &amp; = \dfrac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} = \dfrac{\kappa_0}{\kappa_n} \mu_0 + \dfrac{n}{\kappa_n} \bar{y}\\
\nu_n &amp; = \nu_0 + n\\
\sigma_n^2 &amp; = \dfrac{1}{\nu_n}\left[\nu_0\sigma_0^2 + s^2(n-1) + \dfrac{n\kappa_0}{\kappa_n} (\bar{y} - \mu_0)^2 \right] = \dfrac{1}{\nu_n}\left[\nu_0\sigma_0^2 + \sum_{i=1}^n (y_i-\bar{y})^2 + \dfrac{n\kappa_0}{\kappa_n} (\bar{y} - \mu_0)^2 \right]\\
\end{split}`
$$
]
]

--

- Turns out that the marginal posterior of `\(\mu\)`, that is, `\(\pi(\mu|Y) = \int_0^\infty \pi(\mu, \tau|Y) d\tau\)` is a **t-distribution**. You can derive that distribution if you are interested, we won't spend time on it in class. 



---
## Back to our examples

- .hlight[Pygmalion: questions of interest]
  + Is the average improvement for the accelerated group larger than that for the no growth group?
      - What is `\(\Pr[\mu_A &gt; \mu_N | Y_A, Y_N)\)`?
  + Is the variance of improvement scores for the accelerated group larger than that for the no growth group?
      - What is `\(\Pr[\sigma^2_A &gt; \sigma^2_N | Y_A, Y_N)\)`?
      
--

- .hlight[Job training: questions of interest]
  + Is the average change in annual earnings for the training group larger than that for the no training group?
      - What is `\(\Pr[\mu_T &gt; \mu_N | Y_T, Y_N)\)`?
  + Is the variance of change in annual earnings for the training group larger than that for the no training group?
      - What is `\(\Pr[\sigma^2_T &gt; \sigma^2_N | Y_T, Y_N)\)`?



---
## Mildly informative priors

- We will focus on the Pygmalion study. Follow the same approach for the job training data.

--

- Suppose you have no idea whether students would improve IQ on average. Set `\(\mu_{0A} = \mu_{0N} = 0\)`.

--

- Suppose you don't have any faith in this belief, and think it is the equivalent of having only 1 prior observation in each group. Set `\(\kappa_{0A} = \kappa_{0N} = 1\)`.

--

- Based on the literature, SD of change scores should be around 10 in each group, but still you don't have a lot of faith in this belief. Set 
`\(\nu_{0A} = \nu_{0N} = 1\)` and `\(\sigma^2_{0A} = \sigma^2_{0N} = 100\)`.

--

- Graph priors to see if they accord with your beliefs. Sampling new values of `\(Y\)` from the priors offers a good check.



---
## Recall the Pygmalion data

- Data:
  
  + Accelerated group (A): 20, 10, 19, 15, 9, 18.
  
  + No growth group (N): 3, 2, 6, 10, 11, 5.
--

- Summary statistics:
  
  + `\(\bar{y}_A = 15.2\)`; `\(s_A = 4.71\)`.
  
  + `\(\bar{y}_N = 6.2\)`; `\(s_N = 3.65\)`.



---
## Analysis with mildly informative priors

.block[
.small[
$$
`\begin{split}
\kappa_{nA} &amp; = \kappa_{0A} + n_A = 1 + 6 = 7\\
\kappa_{nN} &amp; = \kappa_{0N} + n_N = 1 + 6 = 7\\
\nu_{nA} &amp; = \nu_{0A} + n_A = 1 + 6 = 7\\
\nu_{nN} &amp; = \nu_{0N} + n_N = 1 + 6 = 7\\
\mu_{nA} &amp; = \dfrac{\kappa_{0A} \mu_{0A} + n_A\bar{y}_A}{\kappa_{nA}} = \dfrac{ (1)(0) + (6)(15.2) }{7} \approx 13.03\\
\mu_{nN} &amp; = \dfrac{\kappa_{0N} \mu_{0N} + n_N\bar{y}_N}{\kappa_{nN}} = \dfrac{ (1)(0) + (6)(6.2) }{7} \approx 5.31\\
\sigma_{nA}^2 &amp; = \dfrac{1}{\nu_{nA}}\left[\nu_{0A}\sigma_{0A}^2 + s^2_A(n_A-1) + \dfrac{n_A\kappa_{0A}}{\kappa_{nA}} (\bar{y}_A - \mu_{0A})^2\right] \\
&amp; = \dfrac{1}{7}\left[(1)(100) + (22.17)(5) + \dfrac{(6)(1)}{(7)} (15.2- 0)^2\right] \approx 58.41\\
\sigma_{nN}^2 &amp; = \dfrac{1}{\nu_{nN}}\left[\nu_{0N}\sigma_{0N}^2 + s^2_N(n_N-1) + \dfrac{n_N\kappa_{0N}}{\kappa_{nN}} (\bar{y}_N - \mu_{0N})^2\right] \\
&amp; = \dfrac{1}{7}\left[(1)(100) + (13.37)(5) + \dfrac{(6)(1)}{(7)} (6.2- 0)^2\right] \approx 28.54\\
\end{split}`
$$
]
]



---
## Analysis with mildly informative priors

- So our joint posterior is
.block[
.small[
$$
`\begin{split}
\mu_A | Y_A, \tau_A &amp; \sim \ \mathcal{N}\left(\mu_{nA}, \dfrac{1}{\kappa_{nA}\tau_A} \right) = \mathcal{N}\left(13.03, \dfrac{1}{7\tau_A} \right)\\
\tau_A | Y_A &amp; \sim \textrm{Gamma}\left(\dfrac{\nu_{nA}}{2}, \dfrac{\nu_{nA} \sigma_{nA}^2}{2}\right) = \textrm{Gamma}\left(\dfrac{7}{2}, \dfrac{7 (58.41)}{2}\right)\\
\mu_N | Y_N, \tau_N &amp; \sim \ \mathcal{N}\left(\mu_{nN}, \dfrac{1}{\kappa_{nN}\tau_N} \right) = \mathcal{N}\left(5.31, \dfrac{1}{7\tau_N} \right)\\
\tau_N | Y_N &amp; \sim \textrm{Gamma}\left(\dfrac{\nu_{nN}}{2}, \dfrac{\nu_{nN} \sigma_{nN}^2}{2}\right) = \textrm{Gamma}\left(\dfrac{7}{2}, \dfrac{7 (28.54)}{2}\right)\\
\end{split}`
$$
]
]



---
## Monte Carlo sampling

- To evaluate whether the accelerated group has larger IQ gains than the normal group, we would like to estimate quantities like `\(\Pr[\mu_A &gt; \mu_N | Y_A, Y_N)\)` which are based on the **marginal posterior** of `\(\mu\)` rather than the **conditional distribution**.

--

- Fortunately, this is easy to do by generating samples of `\(\mu\)` and `\(\sigma^2\)` from their joint posterior.



---
## Monte Carlo sampling

- Suppose we simulate values using the following Monte Carlo procedure:
.block[
.small[
$$
`\begin{split}
\tau^{(1)} &amp; \sim \textrm{Gamma}\left(\dfrac{\nu_{n}}{2}, \dfrac{\nu_{n} \sigma_{n}^2}{2}\right)\\
\mu^{(1)} &amp; \sim \ \mathcal{N}\left(\mu_{n}, \dfrac{1}{\kappa_{n}\tau^{(1)}} \right)\\
\tau^{(2)} &amp; \sim \textrm{Gamma}\left(\dfrac{\nu_{n}}{2}, \dfrac{\nu_{n} \sigma_{n}^2}{2}\right)\\
\mu^{(2)} &amp; \sim \ \mathcal{N}\left(\mu_{n}, \dfrac{1}{\kappa_{n}\tau^{(2)}} \right)\\
&amp; \ \ \vdots \\
&amp; \ \ \vdots \\
&amp; \ \ \vdots \\
\tau^{(m)} &amp; \sim \textrm{Gamma}\left(\dfrac{\nu_{n}}{2}, \dfrac{\nu_{n} \sigma_{n}^2}{2}\right)\\
\mu^{(m)} &amp; \sim \ \mathcal{N}\left(\mu_{n}, \dfrac{1}{\kappa_{n}\tau^{(m)}} \right)\\
\end{split}`
$$
]
]


---
## Monte Carlo sampling

- Note that we are sampling each `\(\mu^{(j)}\)`, `\(j=1,\ldots,m\)`, from its conditional distribution, not from the marginal.

--

- The sequence of pairs `\(\{(\tau, \mu)^{(1)}, \ldots, (\tau, \mu)^{(m)}\}\)` simulated using this method are independent samples from the joint posterior `\(\pi(\mu,\tau | Y)\)`.

--

- Additionally, the simulated sequence `\(\{\mu^{(1)}, \ldots, \mu^{(m)}\}\)` are independent samples from the **marginal posterior distribution**.

--

- While this may seem odd, keep in mind that while we drew the `\(\mu\)`'s as conditional samples, each was conditional on a different value of `\(\tau\)`.

--

- Thus, together they constitute marginal samples of `\(\mu\)`.



---
## Monte Carlo sampling

It is easy to sample from these posteriors:

```r
aA &lt;- 7/2
aN &lt;- 7/2
bA &lt;- (7/2)*58.41
bN &lt;- (7/2)*28.54
muA &lt;- 13.03
muN &lt;- 5.31
kappaA &lt;- 7
kappaN &lt;- 7
tauA_postsample &lt;- rgamma(10000,aA,bA)
thetaA_postsample &lt;- rnorm(10000,muA,sqrt(1/(kappaA*tauA_postsample)))
tauN_postsample &lt;- rgamma(10000,aN,bN)
thetaN_postsample &lt;- rnorm(10000,muN,sqrt(1/(kappaN*tauN_postsample)))
sigma2A_postsample &lt;- 1/tauA_postsample
sigma2N_postsample &lt;- 1/tauN_postsample
```



---
## Monte Carlo sampling

- Is the average improvement for the accelerated group larger than that for the no growth group?
  + What is `\(\Pr[\mu_A &gt; \mu_N | Y_A, Y_N)\)`?
  
  ```r
  mean(thetaA_postsample &gt; thetaN_postsample)
  ```
  
  ```
  ## [1] 0.9685
  ```

--

- Is the variance of improvement scores for the accelerated group larger than that for the no growth group?
  + What is `\(\Pr[\sigma^2_A &gt; \sigma^2_N | Y_A, Y_N)\)`?
  
  ```r
  mean(sigma2A_postsample &gt; sigma2N_postsample)
  ```
  
  ```
  ## [1] 0.8175
  ```
      
--

- &lt;div class="question"&gt;
What can we conclude from this?
&lt;/div&gt;



---

class: center, middle

# What's next? 

### Move on to the readings for the next module!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
