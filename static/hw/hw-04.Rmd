---
title: "Homework 4"
date: "Due: 11:59pm, Thursday, Feb 13"
output: 
  html_document: 
    css: hw.css
    theme: yeti
    toc: true
    fig_caption: false
    toc_float:
      collapsed: false
      smooth_scroll: true
---

## Instructions

Please make sure your final output file is a pdf document. You can submit handwritten solutions for non-programming exercises or type them using R Markdown, LaTeX or any other word processor. All programming exercises MUST be done in R, typed up clearly and with all code attached. Submissions should be made on gradescope: go to Assignments $\rightarrow$ Homework 4.

## Questions



1. Simulate data assuming $\boldsymbol{y_i} = (y_{i1},y_{i2})^T \sim \mathcal{N}_2(\boldsymbol{\theta}, \Sigma)$, $i = 1, \ldots, 100$, with $\boldsymbol{\theta} = (0,0)^T$ and $\Sigma$ chosen so that the marginal variances are one and $\rho = 0.8$.
    + <font color="green">**Part (a):**</font> Estimate the MLE of $(\boldsymbol{\theta}, \Sigma)$. Make two contour plots, one using the true values of $(\boldsymbol{\theta}, \Sigma)$, and the other using the MLEs. Comment on the differences, if any.
    + <font color="green">**Part (b):**</font> Assuming independent normal & inverse-Wishart priors for $\boldsymbol{\theta}$ and $\Sigma$, that is, $\pi(\boldsymbol{\theta}, \Sigma) = \pi(\boldsymbol{\theta}) \pi(\Sigma)$, run Gibbs sampler (hyperparameters up to you but you must justify your choices) to generate posterior samples for $(\boldsymbol{\theta}, \Sigma)$.
    + <font color="green">**Part (c):**</font> Compare Bayes estimates of $(\boldsymbol{\theta}, \Sigma)$ to MLE and truth. Comment on the similarities and differences. How much influence do you think your prior had on the Bayes estimates in particular?
    + <font color="green">**Part (d):**</font> Given $y_{i2}$ values for 50 new (test) subjects, describe in as much detail as possible, how you would use the Gibbs sampler to predict new $y_{i1}$ values, given the $y_{i2}$ values, from the "conditional posterior predictive distribution" of $(y_{i1} | y_{i2})$.

        <font color="red">*You are not expected to derive the maximum likelihood estimators for the two parameters. If you don't know what the MLEs are, you can look up the forms (just the forms!) online.* </font>  


1. <font color="blue">**Continuation of question 1 from homework 5.**</font>
<!-- Recall that in part (d) of question 1 from homework 5, you were asked: given $y_{i,2}$ values for 50 new (test) subjects, describe in as much detail as possible, how you would use the Gibbs sampler to predict new $y_{i,1}$ values, given the $y_{i,2}$ values, from the "conditional posterior predictive distribution" of $(y_{i,1} | y_{i,2})$. -->
Generate 50 "test" subjects from the same sampling distribution as question 1 from homework 5, that is, $\boldsymbol{y_i}^\star = (y_{i,1}^\star,y_{i,2}^\star)^T \sim \mathcal{N}_2(\boldsymbol{\theta}, \Sigma)$, $i = 1, \ldots, 50$, with $\boldsymbol{\theta} = (0,0)^T$ and $\Sigma$ chosen so that the marginal variances are one and $\rho = 0.8$. Keep the $y_{i,2}^\star$ values but treat the $y_{i,1}^\star$ values as unknown/missing (set them to NA but save the "true" values somewhere!).
    
    Now, re-run the Gibbs sampler from the last homework to obtain the posterior samples for $(\boldsymbol{\theta}, \Sigma)$ based only on the 100 "train" subjects from the last homework (that is, $\boldsymbol{y_i} = (y_{i,1},y_{i,2})^T \sim \mathcal{N}_2(\boldsymbol{\theta}, \Sigma)$, $i = 1, \ldots, 100$, with the same $(\boldsymbol{\theta}, \Sigma)$). Using the posterior samples, answer the following questions:
    + <font color="green">**Part (a):**</font> Generate predictive samples of $y_{i,1}^\star$ given each $y_{i,2}^\star$ value you kept, for the 50 test subjects. Show your sampler.  
      <font color="red">*You should view this as a "train --> test" prediction problem rather than a missing data problem on an original data. That is, given the posterior samples of your parameters, and the test values for $y_{i2}^\star$, draw from the posterior predictive distribution of $(y_{i,1}^\star | y_{i,2}^\star, \{(y_{1,1},y_{1,2}), \ldots, (y_{100,1},y_{100,2})\})$,. You may find it useful to think of this in terms of compositional sampling, that is, for each posterior sample of $(\boldsymbol{\theta}, \Sigma)$, sample from $(y_{i,1} | y_{i,2}, \boldsymbol{\theta}, \Sigma)$, which is just from the form of the sampling distribution. Don't incorporate the prediction problem into your original Gibbs sampler!*</font>
      
    + <font color="green">**Part (b):**</font> Using the samples from the predictive density obtained above, obtain $\mathbb{E}[y_{i,1}^\star | y_{i,2}^\star]$ for each of the test subjects, as well as a 95% posterior predictive interval. Make a plot containing all the intervals for each of the 50 subjects. In the plot, indicate where each $\mathbb{E}[y_{i,1}^\star | y_{i,2}^\star]$ falls within each interval.
    + <font color="green">**Part (c):**</font> What is the coverage of the 95% predictive intervals out of sample? That is, how many of the 95% predictive intervals contain the true $y_{i,1}^\star$ values?
    + <font color="green">**Part (d):**</font> Fit a (frequentist) linear model to the original 100 train subjects (see `lm` in R if you have never done this in R, which I hope is not the case), using $y_{i,1}$ as the response variable and $y_{i,2}$ as the predictor. Now, use the results from the fitted model to predict $y_{i,1}^\star$ values given the $y_{i,2}^\star$ values for 50 new test subjects. Show your R code.
    + <font color="green">**Part (e):**</font> Generate and make a plot of the predictive intervals for the frequentist predictions (`lm` predictions are very easy to do in R). Comment on the differences between your results and the Bayesian approach.
    
    

2.  Hoff problem 7.4.

    <font color="red">*You can find the data mentioned in the question here: http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/.* </font>
    
    <font color="red">**For 7.4(d), part (ii) is NOT required but feel free to attempt it for practice; you are only required to submit answers to parts (i) and (iii). For 7.4(d)(i), there is no need to read or complete the entire Exercise 7.1, simply take the Jeffrey's prior (same as the prior on the class slides) and find the corresponding full conditionals.**</font>
    
<!-- We will actual derive the full conditionals in class on Wednesday, February 26, but I STRONGLY suggest attempting it on your own before then, so the in-class results would only confirm your solution. -->

  
  
  
## Grading

15 points.
  